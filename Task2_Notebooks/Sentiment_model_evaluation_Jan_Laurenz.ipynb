{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluation of finetuned pre trained models with sentiment analysis as example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding,  Trainer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import gradio as gr\n",
    "from google.colab import drive\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding,  Trainer\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "\n",
    "# List of pretrained finetuned models\n",
    "model_list = [\"FT:distilbert-base-uncased-finetuned-sst-2-english\", \"FT:english-yelp-sentiment\", \"logistic-regression\" ]\n",
    "\n",
    "\n",
    "# Function to load the pretrained models and evaluate them\n",
    "def pre_trained_model(dropdown=[]):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    path = \"\"\n",
    "\n",
    "    if dropdown == \"FT:distilbert-base-uncased-finetuned-sst-2-english\":\n",
    "      path = \"/content/drive/My Drive/Mein_Modellverzeichnis\"\n",
    "    elif dropdown == \"FT:english-yelp-sentiment\":\n",
    "      path= \"/content/drive/My Drive/Mein_Modellverzeichnis_2\"\n",
    "    elif dropdown == \"logistic-regression\":\n",
    "      return evaluate_logistic_regression()\n",
    "    \n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "\n",
    "    \n",
    "    URL_test = \"https://raw.githubusercontent.com/laurenzbrahner/BigDataTask2/main/data/Sentiment_Test.csv\"\n",
    "    URL_training = \"https://raw.githubusercontent.com/laurenzbrahner/BigDataTask2/main/data/Sentiment_training_extended.csv\"\n",
    "    URL_validation = \"https://raw.githubusercontent.com/laurenzbrahner/BigDataTask2/main/data/Sentiment_Val.csv\"\n",
    "\n",
    "    # Load the CSV files from the URLs\n",
    "    df_train = pd.read_csv(URL_training, sep=\";\")\n",
    "    df_test = pd.read_csv(URL_test, sep=\";\")\n",
    "    df_val = pd.read_csv(URL_validation, sep=\";\")\n",
    "\n",
    "   # 0-3 vs 5 Star Binary mapping\n",
    "    star_mapping = {\n",
    "        0: 0,\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 1\n",
    "    }\n",
    "\n",
    "    df_train['label'] = df_train['label'].map(star_mapping)\n",
    "    df_test['label'] = df_test['label'].map(star_mapping)\n",
    "    df_val['label'] = df_val['label'].map(star_mapping)\n",
    "\n",
    "\n",
    "\n",
    "    raw_datasets = {}\n",
    "    raw_datasets['train'] = Dataset.from_pandas(df_train)\n",
    "    raw_datasets['test'] = Dataset.from_pandas(df_test)\n",
    "    raw_datasets['val'] = Dataset.from_pandas(df_val)\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenizer_1 = tokenizer\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer_1(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "\n",
    "\n",
    "    # use tokenize_function on each dataset to tokenize the datasets\n",
    "    tokenized_datasets = {x: raw_datasets[x].map(tokenize_function, batched=True) for x in raw_datasets}\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "    trainer = Trainer(model=model, data_collator=data_collator)\n",
    "\n",
    "    predictions = trainer.predict(tokenized_datasets[\"val\"])\n",
    "\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate Confusion Matrix\n",
    "    cm = confusion_matrix(predictions.label_ids, preds)\n",
    "\n",
    "    # map Confusion Matrix Labels as Strings \n",
    "    labels = [\"less than 5 stars\", \"5 stars\"]\n",
    "\n",
    "    # visualize Confusion Matrix \n",
    "    # Erstellen eines Figure- und Axes-Objekts\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "    # Generate Heatmap-Plot\n",
    "    sns.heatmap(cm, annot=True, fmt='g', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "\n",
    "    # Matrix labels\n",
    "    ax.set_xlabel('predicted classes')\n",
    "    ax.set_ylabel('actual classes')\n",
    "\n",
    "    # Speichern des Figure-Objekts in einer Variablen\n",
    "    heatmap_plot = fig\n",
    "\n",
    "    true_labels = predictions.label_ids\n",
    "\n",
    "  # Predictions\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "  # Accuracy\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "\n",
    "  # Error Rate\n",
    "    error_rate = 1 - accuracy\n",
    "\n",
    "  # Precision, Recall, F1-Measure, and Support (we won't use support here)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='macro')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Return the metrics\n",
    "    metrics_data = {\n",
    "        \"Metric\": [\"Accuracy\", \"Error Rate\", \"Precision\", \"Recall\", \"F1-Measure\"],\n",
    "        \"Value\": [accuracy, error_rate, precision, recall, f1]\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "\n",
    "    return metrics_df, heatmap_plot;\n",
    "\n",
    "\n",
    "# Function to evaluate the logistic regression model\n",
    "def evaluate_logistic_regression():\n",
    "    URL_test = \"https://raw.githubusercontent.com/laurenzbrahner/BigDataTask2/main/data/Sentiment_Test.csv\"\n",
    "    URL_training = \"https://raw.githubusercontent.com/laurenzbrahner/BigDataTask2/main/data/Sentiment_training_extended.csv\"\n",
    "    URL_validation = \"https://raw.githubusercontent.com/laurenzbrahner/BigDataTask2/main/data/Sentiment_Val.csv\"\n",
    "\n",
    "    # Load the CSV files from the URLs\n",
    "    df_train = pd.read_csv(URL_training, sep=\";\")\n",
    "    df_test = pd.read_csv(URL_test, sep=\";\")\n",
    "    df_val = pd.read_csv(URL_validation, sep=\";\")\n",
    "\n",
    "    # 0-3 vs 5 Star Binary mapping\n",
    "    star_mapping = {\n",
    "        0: 0,\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 1\n",
    "    }\n",
    "\n",
    "    df_train['label'] = df_train['label'].map(star_mapping)\n",
    "    df_test['label'] = df_test['label'].map(star_mapping)\n",
    "    df_val['label'] = df_val['label'].map(star_mapping)\n",
    "\n",
    "    X_train = df_train['text'].values\n",
    "    y_train = df_train['label'].values\n",
    "    X_val = df_val['text'].values\n",
    "    y_val = df_val['label'].values\n",
    "\n",
    "    # CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(X_train)\n",
    "\n",
    "    # Transform data\n",
    "    X_train = vectorizer.transform(X_train)\n",
    "    X_test = vectorizer.transform(df_test[\"text\"].values)\n",
    "    X_val = vectorizer.transform(X_val)\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(max_iter=3500)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # Prediction\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # Calculate Confusion Matrix\n",
    "    cm = confusion_matrix(df_test[\"label\"], y_pred)\n",
    "\n",
    "    # Visualize Confusion Matrix\n",
    "    labels = [\"less than 5 stars\", \"5 stars\"]\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "    ax.set_xlabel('Predicted Classes')\n",
    "    ax.set_ylabel('Actual Classes')\n",
    "    heatmap_plot = fig\n",
    "\n",
    "    # Calculate Metrics\n",
    "    accuracy = accuracy_score(df_test[\"label\"], y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(df_test[\"label\"], y_pred, average='macro')\n",
    "\n",
    "    # Return the metrics\n",
    "    metrics_data = {\n",
    "        \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Measure\"],\n",
    "        \"Value\": [accuracy, precision, recall, f1]\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "    return metrics_df, heatmap_plot\n",
    "\n",
    "\n",
    "\n",
    "# Create the dropdown menu\n",
    "dropdown = gr.Dropdown(model_list, label=\"Choose a pretrained model to view its evaluation measures.\")\n",
    "\n",
    "\n",
    "# Create the interface\n",
    "demo = gr.Interface(\n",
    "    fn=pre_trained_model,\n",
    "    inputs=dropdown,\n",
    "    outputs=[gr.Dataframe(label=\"Measures\"), gr.Plot(label=\"Barplot\")], \n",
    "    title=\"Fine-Tuned-Models -- evaluation and comparison\",\n",
    "    allow_flagging=\"never\"\n",
    "    )\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch(debug=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
