{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from google.colab import drive\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding,  Trainer\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "\n",
    "# List of pretrained models\n",
    "model_list = [\"rubert-base-cased-finetuned\", \"DeBERTa-v3-base-finetuned\", \"rubert-base-cased-not_finetuned\", \"DeBERTa-v3-base-not_finetuned\" ]\n",
    "\n",
    "# Function to load the pretrained model and evaluate it on the validation set\n",
    "def pre_trained_model(dropdown=[]):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    path = \"\"\n",
    "    \n",
    "    if dropdown == \"rubert-base-cased-finetuned\":\n",
    "      path = \"/content/drive/My Drive/Mein_Modellverzeichnis_1\"\n",
    "    elif dropdown == \"DeBERTa-v3-base-finetuned\":\n",
    "      path= \"/content/drive/My Drive/Mein_Modellverzeichnis_2\"\n",
    "    elif dropdown == \"rubert-base-cased-not_finetuned\":\n",
    "      path=\"cointegrated/rubert-base-cased-nli-threeway\"\n",
    "    elif dropdown == \"DeBERTa-v3-base-not_finetuned\":\n",
    "      path=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "      \n",
    "    \n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(path, num_labels=8, ignore_mismatched_sizes=True)\n",
    "\n",
    "    # Load the data\n",
    "    URL_test = \"https://raw.githubusercontent.com/laurenzbrahner/BigDataTask2/main/data/Recipes_Test.csv\"\n",
    "    URL_training = \"https://raw.githubusercontent.com/laurenzbrahner/BigDataTask2/main/data/Recipes_Training.csv\"\n",
    "    URL_validation = \"https://raw.githubusercontent.com/laurenzbrahner/BigDataTask2/main/data/Recipes_Validation.csv\"\n",
    "\n",
    "    # Load the CSV files from the URLs\n",
    "    df_train = pd.read_csv(URL_training, sep=\";\")\n",
    "    df_test = pd.read_csv(URL_test, sep=\";\")\n",
    "    df_val = pd.read_csv(URL_validation, sep=\";\")\n",
    "\n",
    "    # Map the cuisines to numbers\n",
    "    cuisine_mapping = {\n",
    "        \"cajun_creole\": 0,\n",
    "        \"chinese\": 1,\n",
    "        \"french\": 2,\n",
    "        \"indian\": 3,\n",
    "        \"italian\": 4,\n",
    "        \"mexican\": 5,\n",
    "        \"southern_us\": 6,\n",
    "        \"thai\": 7\n",
    "    }\n",
    "\n",
    "    #df_train['cuisine'] = df_train['cuisine'].map(cuisine_mapping)\n",
    "    df_test['cuisine'] = df_test['cuisine'].map(cuisine_mapping)\n",
    "    #df_val['cuisine'] = df_val['cuisine'].map(cuisine_mapping)\n",
    "\n",
    "\n",
    "    # Load the datasets\n",
    "    raw_datasets = {}\n",
    "    raw_datasets['train'] = Dataset.from_pandas(df_train)\n",
    "    raw_datasets['test'] = Dataset.from_pandas(df_test)\n",
    "    raw_datasets['val'] = Dataset.from_pandas(df_val)\n",
    "\n",
    "    # Load the model and the tokenizer\n",
    "    checkpoint = model\n",
    "    tokenizer_1 = tokenizer\n",
    "\n",
    "    # Tokenize the ingredients and add the cuisines as labels\n",
    "    def tokenize_function(examples):\n",
    "        # Tokenisieren der Zutaten und HinzufÃ¼gen der 'cuisines' als Labels\n",
    "        tokenized_inputs = tokenizer_1(examples[\"ingredients\"], truncation=True, padding=\"max_length\")\n",
    "        tokenized_inputs[\"labels\"] = examples[\"cuisine\"]\n",
    "        return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_datasets = {x: raw_datasets[x].map(tokenize_function, batched=True) for x in raw_datasets}\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "    trainer = Trainer(model=model, data_collator=data_collator)\n",
    "\n",
    "    predictions = trainer.predict(tokenized_datasets[\"val\"])\n",
    "\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "    # Confusion Matrix calculate\n",
    "    cm = confusion_matrix(predictions.label_ids, preds)\n",
    "\n",
    "    # Labels for Mapping\n",
    "    labels = [\"cajun_creole\", \"chinese\", \"french\", \"indian\", \"italian\", \"mexican\", \"southern_us\", \"thai\"]\n",
    "\n",
    "    # Confusion Matrix visualisieren\n",
    "    # Create a figure and an axes\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "    # Create the heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='g', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "\n",
    "    # laebl the axes\n",
    "    ax.set_xlabel('predicted classes')\n",
    "    ax.set_ylabel('actual classes')\n",
    "\n",
    "    # save in variable\n",
    "    heatmap_plot = fig\n",
    "\n",
    "    true_labels = predictions.label_ids\n",
    "\n",
    "    # Predictions\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "\n",
    "    # Error Rate\n",
    "    error_rate = 1 - accuracy\n",
    "\n",
    "    # Precision, Recall, F1-Measure, and Support (we won't use support here)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='macro')\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # DataFrame  \n",
    "    metrics_data = {\n",
    "        \"Metric\": [\"Accuracy\", \"Error Rate\", \"Precision\", \"Recall\", \"F1-Measure\"],\n",
    "        \"Value\": [accuracy, error_rate, precision, recall, f1]\n",
    "    }\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "\n",
    "    # Confusion Matrix \n",
    "    cm = confusion_matrix(predictions.label_ids, preds)\n",
    "\n",
    "    # Labels for Mapping\n",
    "    labels = [\"cajun_creole\", \"chinese\", \"french\", \"indian\", \"italian\", \"mexican\", \"southern_us\", \"thai\"]\n",
    "\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    # find the most frequent misclassification per class\n",
    "    for i in range(cm.shape[0]):\n",
    "        row = cm[i].copy()\n",
    "        row[i] = 0\n",
    "        max_value = np.max(row)\n",
    "        \n",
    "        if max_value > 0:\n",
    "            j = np.argmax(row)\n",
    "            data.append({\"Actual\": labels[i], \"Predicted\": labels[j], \"Count\": max_value})\n",
    "\n",
    "    # convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # sort by count\n",
    "    df_sorted = df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "    # Altair Plot\n",
    "    chart = alt.Chart(df_sorted).mark_bar().encode(\n",
    "        x=alt.X('Actual:N', sort='-y', title=\"Actual Class\"),\n",
    "        y=alt.Y('Count:Q', title=\"Error Count\"),\n",
    "        color=alt.Color(field='Predicted', type='nominal',legend=alt.Legend(title='Predicted Class')),\n",
    "        tooltip=['Actual', 'Predicted', 'Count']\n",
    "    ).properties(\n",
    "        width=600,\n",
    "        height=400,\n",
    "        title=\"Most frequent misclassification per Class\"\n",
    "    ).configure_title(\n",
    "        fontSize=25,\n",
    "        anchor='start'\n",
    "    ).configure_axis(\n",
    "        labelFontSize=12,\n",
    "        titleFontSize=24,\n",
    "        titleColor='gray',\n",
    "        labelColor='gray',\n",
    "        titlePadding=7,\n",
    "        grid=False\n",
    "    ).configure_view(\n",
    "        strokeWidth=0,\n",
    "    ).configure_axisX(\n",
    "        labelAngle=0,\n",
    "        titleAnchor='start'\n",
    "    ).configure_axisY(\n",
    "        grid=False,\n",
    "        titleAnchor='end',\n",
    "        titleFontSize=20\n",
    "    )\n",
    "\n",
    "\n",
    "    return metrics_df, heatmap_plot, chart\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "dropdown = gr.Dropdown(model_list, label=\"Choose a pretrained model to view its evaluation measures\")\n",
    "\n",
    "\n",
    "# Interface\n",
    "demo = gr.Interface(\n",
    "    fn=pre_trained_model,\n",
    "    inputs=dropdown,\n",
    "    outputs=[gr.Dataframe(label=\"Metrics\"), gr.Plot(label=\"Heatmap\"), gr.Plot(label=\" Misclasification Barplot\")], \n",
    "    title=\"Fine-Tuned-Models -- evaluation and comparison\",\n",
    "    allow_flagging=\"never\"\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "demo.launch()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
