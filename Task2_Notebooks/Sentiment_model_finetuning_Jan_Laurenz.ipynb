{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finetuning different pretrained models to perform sentiment analysis of yelp reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed python packages\n",
    "!pip install accelerate\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install pandas\n",
    "!pip install altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_test = \"https://raw.githubusercontent.com/laurenzbrahner/BigDataTask2/main/data/Sentiment_Test.csv\"\n",
    "URL_training = \"https://raw.githubusercontent.com/laurenzbrahner/BigDataTask2/main/data/Sentiment_training_extended.csv\"\n",
    "URL_val = \"https://raw.githubusercontent.com/laurenzbrahner/BigDataTask2/main/data/Sentiment_Val.csv\"\n",
    "\n",
    "# Load Data\n",
    "df_train = pd.read_csv(URL_training, sep=\";\")\n",
    "df_test = pd.read_csv(URL_test, sep=\";\")\n",
    "df_val = pd.read_csv(URL_val, sep=\";\")\n",
    "\n",
    "# 0-3 vs 5 Star binary mapping\n",
    "star_mapping = {\n",
    "    0: 0,\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 0,\n",
    "    4: 1\n",
    "}\n",
    "\n",
    "df_train['label'] = df_train['label'].map(star_mapping)\n",
    "df_test['label'] = df_test['label'].map(star_mapping)\n",
    "df_val['label'] = df_val['label'].map(star_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution\n",
    "df_train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose amount to delete from df to balance the distribution\n",
    "indices_to_remove = df_train[df_train['label'] == 0].sample(1300, random_state=42).index\n",
    "\n",
    "# delete from df\n",
    "df_train = df_train.drop(indices_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter model name as checkpoint\n",
    "checkpoint = \"gilf/english-yelp-sentiment\"\n",
    "# keep tokenizer same as model, only change if model has problems with tokenizer\n",
    "checkpoint_tokenizer= \"gilf/english-yelp-sentiment\"\n",
    "\n",
    "# other used models for finetuning \\|/\n",
    "#\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_tokenizer)\n",
    "\n",
    "# Define datacollator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# process loaded data\n",
    "raw_datasets = {}\n",
    "raw_datasets['train'] = Dataset.from_pandas(df_train)\n",
    "raw_datasets['test'] = Dataset.from_pandas(df_test)\n",
    "raw_datasets['val'] = Dataset.from_pandas(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doublecheck if data is balanced\n",
    "df_train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,\n",
    "                                                           num_labels=2,\n",
    "                                                           ignore_mismatched_sizes=True)\n",
    "# define training arguments\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"steps\",\n",
    "                                  num_train_epochs=5,metric_for_best_model=\"accuracy\",\n",
    "                                   load_best_model_at_end=True,)\n",
    "\n",
    "# tokenize data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "tokenized_datasets = {x: y.map(tokenize_function, batched=True) for x, y in raw_datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define compute metrics\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    accuracy = (preds == labels).mean()\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1\": macro_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
